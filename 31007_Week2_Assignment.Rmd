---
title: "MScA Statistical Analysis 31007, Week 2 Assignment"
author: 'Mark Preston, Student ID: 12191901'
date: "April 3, 2018"
output: 
  html_document: 
    fig_height: 7
    fig_width: 9
---

***

##1 Generate uniformly distributed random numbers

###1.1 Use runif()

Function runif(N,a,b) simulates N pseudo-random numbers uniformly distributed on [a,b].


```{r running initial sample}
set.seed(15)
Sample <- runif(1000, 0, 1)
```

###1.2 Simulate Uniform Random Sample on [0,1] Using Random.org.

From Random.org you can download binary sequence using library `random`. This library is an interface to the web service of Random.org.

```{r installing and loading random package, warning=FALSE}
#install.packages("random")
library(random)

nFlips <- 1000
random_data <- randomNumbers(n = nFlips, min = 0, max = 1, col = 1,
                           base = 2, check = TRUE)
head(random_data, 10)
```

###1.3 Downloading data from Random.org directly

Since the `random` library worked, I didn't download any additional data from Random.org.

###1.4 Turning binary sequence to uniform random numbers

Turn your sequence of {0,1} into uniform random numbers, [0,1].

Create function that turns a sequence of zeros and ones of length n into decimal form.

```{r bits to integer function}
bitsToInt<-function(x) {
    packBits(rev(c(rep(FALSE, 32-length(x)%%32), as.logical(x))), "integer")
}

bitsToInt(c(1,1,1,1,1,0))
```

Turn the sequence of zeros and ones `dataFromRandom` of length 1000 into a matrix with 10 columns and 100 rows.

```{r vector into 10 x 100 matrix}
binary_matrix <- matrix(data = binary_matrix, ncol = 10)

head(binary_matrix)
```

Transform each row of the matrix into decimal format using `bitsToInt` and divide the numbers by 2 to the power 10 to make real numbers in [0,1].

```{r random data decimals}
decimal_sample <-apply(binary_matrix, 1, bitsToInt)/2^10

head(decimal_sample)
```

All numbers in `Decimal.Sample` are between 0 and 1. This is your own equivalent of the sample obtained by `runif()`.

```{r verifying range of decimal sample- numbers between 0 and 1}
range(decimal_sample)
```

***

##2 Test random number generators

###2.1 Test uniformity of distribution of both random number generators

####2.1.1 Using Sample generated by runif(): Analyze what was simulated by first looking at the histogram. What does the histogram tell you about the distribution? Is it consistent with the goal of simulation?

The goal of the simulation is to highlight how `runif` generates pseudo-random numbers and subsequently, observe their distribution to ensure they conform to RNG principles. Here, it appears that there isn't any identifiable pattern in the histogram and the distribution is generally uniform. 

```{r sample histogram}
library(ggplot2)
library(dplyr)

sample_df <- as.data.frame(Sample)

sample_histogram <- sample_df %>%
  mutate(cut = cut(Sample, breaks = 10, labels = c(.1, .2, .3, .4, .5,
                                                   .6, .7, .8, .9, 1))) %>%
  count(cut) %>%
  rename(counts = n) %>%
  mutate(density = counts / 1000 * 10)

sample_histogram %>%
  ggplot(aes(cut, counts)) +
  geom_col() +
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  labs(title = "Sample of random numbers (length 1000) from runif plot- seems to conform to RNG principles")
```

There is some variation, as seen by the uneven counts and densities, but generally each break is close. Given this, it seems that the simulation is line with random principles, though some further testing might be revealing.

As a note, I prefer using `dplyr` and `ggplot` so the provided histogram numbers differ slightly from the ones here (due to how the samples get rounded). However, this doesn't change any of the underlying statistics dramatically. 

```{r histogram statistics}
library(kableExtra)

custom_kable <- function(x){
  kable(x, format = "html") %>%
  kable_styling(bootstrap_options = "striped")
}

custom_kable(sample_histogram)
```

The next chunk provides an estimate for the density mean and standard deviation for the histogram.

```{r histogram density mean and sd}
density_stats <- sample_histogram %>%
  summarise(density.mean = mean(density),
            density.sd = sd(density),
            upper.tail = density.mean + 1.96 * density.sd,
            lower.tail = density.mean - 1.96 * density.sd)

count_stats <- sample_histogram %>%
  summarise(counts.mean = mean(counts),
            counts.sd = sd(counts),
            upper.tail = counts.mean + 1.96 * counts.sd,
            lower.tail = counts.mean - 1.96 * counts.sd)

kable_left <- function(x){
  kable(x, format = "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
}

kable_left(density_stats)
```

####What does the graph tell you about the observed distribution?

The plot highlights that all the values are within the 95% confidence interval for the distribution. This lends further evidence to the claim that the numbers are truly randomized between 0 and 1 because the expectation would be a density of 1 per bin (evenly distributed). The confidence interval captures this by showing that despite some variation, each bin is within the acceptable 95% bounds.

```{r sample histogram with mean and confidence intervals}
sample_histogram %>%
  ggplot(aes(cut, counts)) +
  geom_col(fill = "lightgray") +
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  geom_hline(yintercept = count_stats$counts.mean, colour = "#FF7F00", size = 1.3) +
  geom_hline(yintercept = count_stats$upper.tail, colour = "royalblue2",
             linetype = "dashed", size = 1.3) +
  geom_hline(yintercept = count_stats$lower.tail, colour = "royalblue2",
             linetype = "dashed", size = 1.3) +
  labs(title = "Plot shows that variables seem to conform to RNG principles- All values with in 95% confidence interval",
       subtitle = "Orange line represents mean (100) and both blue lines are upper and lower confidence bounds (118 and 82.3)",
       caption = "Data: 1000 random numbers generated using runif",
       x = NULL)
```

####What do you conclude about the estimated distribution from the moments?

A uniform distribution of random numbers between 0 and 1 should have theoretical values of .5 and .25 for moment one (expected mean) and moment two (expected variance). These are good starting points for comparing to the actual moments here because they allow a pure theoretical comparison to the sampled random numbers from `runif`. If the numbers are similiar, there is further evidence that the RNG simulates values correctly.

As seen, the actual compares well to the theoretical moments. The expected means are very close (about .016 difference) and the expected variance is close as well, though with a slightly larger difference (about .166). This variance was visiable in the random number plots given the bars for each break were not perfectly equal. As such, the larger deviation from the theoertical value here is not overly surprising. Overall though, all of this lends credibility to the RNG since the actual moments align well with the theoretical values.  

```{r sample moments}
kable_left(
  sample_df %>%
    summarise(first.moment = mean(Sample),
              theoretical.first = .5,
              second.moment = var(Sample),
              theoretical.second = .25)
)
```

####What do you think is the best way of estimating a uniform distribution over an unknown interval?

The best way to estimate the uniform distribution over an unknown interval is to assume it conforms to the expected values for the distribution. This means that it has a first moment of .5, a second moment of .25, and so on. Additionally, this means that the values will be about evenly distributed between 0 and 1 so any quantiles should be 0, .25, .5 (first moment), .75 and 1. All of these values provide sound estimations when working with an unknown uniform distribution interval.

```{r summary output for sample df}
sample_stats <- sample_df %>%
  summarise(sample.min = min(Sample),
            sample.25.Q = quantile(Sample, probs = .25),
            sample.mean = mean(Sample),
            sample.median = median(Sample),
            sample.75.Q = quantile(Sample, probs = .75),
            sample.max = max(Sample))

kable_left(sample_stats)
```

These estimation principles can be seen on display in a scatter plot below. The mean and quantiles are very close to the theoertical values. Another interesting feature with the grid here is seeing how each square has a reasonably similiar number of points in it but, without any discernable pattern.

```{r random number scatterplot}
sample_df %>%
  mutate(number = 1:1000) %>%
  ggplot(aes(number, Sample)) +
  geom_point() +
  geom_hline(yintercept = sample_stats$sample.mean, colour = "#FF7F00", 
             size = 1.3, alpha = .6) +
  geom_hline(yintercept = sample_stats$sample.25.Q, colour = "royalblue2",
             linetype = "dashed", size = 1.3, alpha = .6) +
  geom_hline(yintercept = sample_stats$sample.75.Q, colour = "royalblue2",
             linetype = "dashed", size = 1.3, alpha = .6) +
  labs(title = "Expected values can be used to estimate unknown uniform distribution- Sample values are all very close to theoretical",
       subtitle = "Orange line represents mean (.51) and both blue lines are 25th and 75th quantiles (.27 and .76)",
       caption = "Data: 1000 random numbers generated using runif")
```

####2.1.2 Repeat the same steps to test uniformity of the sample from Random.org

Below is the same treatment for the Random.org decimal sample. As with before, the goal is to assess if the Random.org data sample is correctly generating pseudo-random numbers by bserving their distribution to ensure they conform to RNG principles. 

The sample has more variation than the `runif` values but, this isn't too surprising since it's a smaller sample size. There doesn't appear to be any identifiable pattern in the histogram but, there is some wide variaition so some more testing wil help determine if it meets the uniform distribution standards.

```{r sample histogram for random.org}
random_df <- data.frame(Sample = decimal_sample)

sample_histogram <- random_df %>%
  mutate(cut = cut(Sample, breaks = 10, labels = c(.1, .2, .3, .4, .5,
                                                   .6, .7, .8, .9, 1))) %>%
  count(cut) %>%
  rename(counts = n) %>%
  mutate(density = counts / 1000 * 100)

sample_histogram %>%
  ggplot(aes(cut, counts)) +
  geom_col() +
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  labs(title = "Sample of random numbers (length 100) from random.org plot")
```

There is some variation, as seen by the uneven counts and densities, and each break is variable ranging from .3 to 1.8. At a glance though, the density looks like it has an even mean (1).

```{r histogram statistics for random.org}
custom_kable(sample_histogram)
```

The next chunk provides an estimate for the density mean and standard deviation for the histogram. My intuition looks good here as the mean is indeed 1. Not surprisingly, there is a much larger standard deviaiton here when compared to the `runif` sample.

```{r histogram density mean and sd for random.org}
density_stats <- sample_histogram %>%
  summarise(density.mean = mean(density),
            density.sd = sd(density),
            upper.tail = density.mean + 1.96 * density.sd,
            lower.tail = density.mean - 1.96 * density.sd)

count_stats <- sample_histogram %>%
  summarise(counts.mean = mean(counts),
            counts.sd = sd(counts),
            upper.tail = counts.mean + 1.96 * counts.sd,
            lower.tail = counts.mean - 1.96 * counts.sd)

kable_left(density_stats)
```

####What does the graph tell you about the observed distribution?

The plot highlights that all the values are within the 95% confidence interval for the distribution. This lends further evidence to the claim that the numbers are truly randomized between 0 and 1 because the expectation would be a density of 1 per bin (evenly distributed). The confidence interval captures this by showing that despite some variation, each bin is within the acceptable 95% bounds.

```{r sample histogram with mean and confidence intervals for random.org}
sample_histogram %>%
  ggplot(aes(cut, counts)) +
  geom_col(fill = "lightgray") +
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  geom_hline(yintercept = count_stats$counts.mean, colour = "#FF7F00", size = 1.3) +
  geom_hline(yintercept = count_stats$upper.tail, colour = "royalblue2",
             linetype = "dashed", size = 1.3) +
  geom_hline(yintercept = count_stats$lower.tail, colour = "royalblue2",
             linetype = "dashed", size = 1.3) +
  labs(title = "Plot shows that variables seem to conform to RNG principles- All values with in 95% confidence interval",
       subtitle = "Orange line represents mean (10) and both blue lines are upper and lower confidence bounds (1.28 and 18.7)",
       caption = "Data: 100 random numbers from Random.org converted to 0-1 range",
       x = NULL)
```

####What do you conclude about the estimated distribution from the moments?

As seen, the actual compares well to the theoretical moments. The expected means close (about .05 difference) and the expected variance is close as well, though with a slightly larger difference (about .167). This variance was visible in the random number plots given the bars for each break were not perfectly equal. As such, the larger deviation from the theoertical value here is not overly surprising. Overall though, all of this lends credibility to the Random.org numbers since the actual moments align well with the theoretical values.  

```{r sample moments for random.org}
kable_left(
  random_df %>%
    summarise(first.moment = mean(Sample),
              theoretical.first = .5,
              second.moment = var(Sample),
              theoretical.second = .25)
)
```


```{r random.org number scatterplot}
sample_stats <- random_df %>%
  summarise(sample.min = min(Sample),
            sample.25.Q = quantile(Sample, probs = .25),
            sample.mean = mean(Sample),
            sample.median = median(Sample),
            sample.75.Q = quantile(Sample, probs = .75),
            sample.max = max(Sample))

random_df %>%
  mutate(number = 1:100) %>%
  ggplot(aes(number, Sample)) +
  geom_point() +
  geom_hline(yintercept = sample_stats$sample.mean, colour = "#FF7F00", 
             size = 1.3, alpha = .6) +
  geom_hline(yintercept = sample_stats$sample.25.Q, colour = "royalblue2",
             linetype = "dashed", size = 1.3, alpha = .6) +
  geom_hline(yintercept = sample_stats$sample.75.Q, colour = "royalblue2",
             linetype = "dashed", size = 1.3, alpha = .6) +
  labs(title = "Ranodm.org scatterplot- values are more variable than the runif numbers",
       subtitle = "Orange line represents mean (.45) and both blue lines are 25th and 75th quantiles (.20 and .69)",
       caption = "Data: 100 random numbers from Random.org converted to 0-1 range")
```

###2.2 Test independence of the sequence of zeros and ones

####2.2.1 Turning point test

Turning point test is used to check if a sequence of numbers is i.i.d. (independent identically distributed).

The test is based on the number of turning points in the sequence.

The number of turning points is the number of maxima and minima in the series.

Let T be the number of turning points in a sample of length n large enough. The test is performed by `turning.point.test()` in the `randtests` package.

In this case, the p-value (.87) is very large and therefore, the null hypothesis is not rejected. With this, it can be concluded that the sample meets the Turning Point test randomness criteria. 

```{r random.org testing, message=FALSE, warning=FALSE}
#install.packages(randtests)
library(randtests)

turning.point.test(decimal_sample)
```

####2.2.2 Test frequency by Monobit test

To perform Monobit test you need to transform your {0,1} sample into {-1,1}.

Illustrate the test on the sequence simulated in the previous lecture.

We created the sequence of coin tosses. Erfc is the complimentary error function, a special function complimentary to error function erf=1-erfc.

Both functions can be easily calculated in R with the help of pnorm:

```{r monobit testing set-up}
monobit_random <- (random_data - .5) * 2

erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1

erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)
```

To test if sequence Ri check the value erfc(S), review if the P-value or erfc(S) is less or equal than 0.01. if so, the sequence fails the test. the Random.org data passes as seen below.

```{r monobit testing}
erfc(abs(sum(monobit_random)/sqrt(2 * nFlips)))
```

Each run can be checked as well. As seen, all null hypotheses are rejected.

```{r erfc plotting test}
erfc_df <- data.frame(
  test = 1:20,
  erfc.p.value = erfc(abs(apply(matrix(monobit_random, ncol = 50), 1, sum))/sqrt(2 * 50))
)


erfc_df %>%
  mutate(significance.threshold = ifelse(erfc.p.value <= .01, 
                                         "non-random", "random")) %>%
  ggplot(aes(test, erfc.p.value, colour = significance.threshold)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0.01, colour = "red", size = 1.3) +
  scale_color_manual(values = c("#00EE76")) +
  labs(title = "All erfc tests are above p-value threshold, though point 20 is close")
```

***

##3 Invent a random number generator

Think about possible sources of true or pseudo-random sequences of {0,1} and choose one.

Conduct the tests described in the previous section.

###Description of your random number generator


###Generated sequence

###Results of the uniformity test

###Results of the frequency test

###Results of the turning point test

